{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from skimage.transform import resize\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Downsample using order 8 Chebyshev type I filter\n",
    "def downsample(data):\n",
    "    return scipy.signal.decimate(data, 6, axis = 0)\n",
    "\n",
    "def prepare_data(mat):\n",
    "    # Left footstep\n",
    "    dataL = mat['dataL']\n",
    "    # Right footstep\n",
    "    dataR = mat['dataR']  \n",
    "    return (dataL, dataR)\n",
    "\n",
    "def calGRF(data):\n",
    "    return np.add.accumulate(data)\n",
    "\n",
    "def process_foot(data):\n",
    "    Tmax = 1600\n",
    "    data = data[:Tmax]\n",
    "    data = downsample(data)\n",
    "    grf = calGRF(data)\n",
    "    s_avg = data.mean(axis=1)\n",
    "    s_upper = np.amax(data,axis=1)\n",
    "    s_lower = np.amin(data,axis=1)\n",
    "    grf_t = grf.mean(axis=1)\n",
    "    grf_t_max = np.max(grf_t)\n",
    "    grf_t /= grf_t_max\n",
    "    processed = np.append(s_avg, [grf_t,s_upper,s_lower])\n",
    "    return processed \n",
    "\n",
    "def process_matlab(mat):\n",
    "    dataL, dataR = prepare_data(mat)\n",
    "    footL, footR = process_foot(dataL), process_foot(dataR)\n",
    "    processed = np.append(footL,footR)\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "import scipy.io\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_files_path = 'SFootBD/IndexFiles/'\n",
    "data_files_path = 'SFootBD/SFootBD/'\n",
    "\n",
    "train_list_file = index_files_path + 'LstTrain'\n",
    "imposter_list_file = index_files_path + 'LstImp'\n",
    "test_evaluation_list_file = index_files_path + 'LstTestEvaluation'\n",
    "test_validation_list_file = index_files_path + 'LstTestValidation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(filename):\n",
    "    mat_file = os.path.join(data_files_path, filename + '.mat')\n",
    "    mat = scipy.io.loadmat(mat_file)\n",
    "    return process_matlab(mat)\n",
    "\n",
    "def load_data_file(index_file):\n",
    "    return pd.read_csv(index_file, sep = ' ', header = None)\n",
    "\n",
    "def load_dataset(data_list, clients):    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for item in data_list.itertuples():\n",
    "        X_item = load_features(item[2])\n",
    "        y_item = item[1]\n",
    "        \n",
    "        matches = np.where(clients == y_item)[0]\n",
    "        \n",
    "        X.append(X_item)\n",
    "        # clients.shape[0] becomes imposter class\n",
    "        y.append(matches[0] if matches.shape[0] != 0 else clients.shape[0])\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = load_data_file(train_list_file)\n",
    "\n",
    "clients = np.unique(train_index[0])\n",
    "\n",
    "X_train, y_train = load_dataset(train_index, clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imposter_index = load_data_file(imposter_list_file)\n",
    "\n",
    "X_imposter, y_imposter = load_dataset(imposter_index, clients)\n",
    "\n",
    "X_train = np.append(X_train, X_imposter, axis = 0)\n",
    "y_train = np.append(y_train, y_imposter, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set is too big\n",
    "val_index = load_data_file(test_validation_list_file)\n",
    "\n",
    "X_val, y_val = load_dataset(val_index, clients)\n",
    "\n",
    "X_train = np.append(X_val, X_imposter, axis = 0)\n",
    "y_train = np.append(y_val, y_imposter, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('std_scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('model',\n",
       "                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"std_scaler\", StandardScaler()),\n",
    "    #(\"model\", OneVsRestClassifier(SVC(kernel='rbf')))\n",
    "    (\"model\", SVC(kernel='rbf'))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set is too big\n",
    "eval_index = load_data_file(test_evaluation_list_file)\n",
    "\n",
    "X_test, y_test = load_dataset(eval_index, clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7072727272727273"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-024c14e49e85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mbob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bob'"
     ]
    }
   ],
   "source": [
    "#import bob.measure #doesn't work on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
